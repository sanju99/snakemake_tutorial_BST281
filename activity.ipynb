{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cfcdf1-b63a-4666-8b5c-c345eaede17f",
   "metadata": {},
   "source": [
    "# Building Bioinformatics Workflows with SnakeMake\n",
    "\n",
    "## BST 281 Guest Lecture\n",
    "### April 28, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef766a-077c-4d9d-98df-216c2732c42b",
   "metadata": {},
   "source": [
    "# Why Learn Workflow Languages?\n",
    "\n",
    "<ul>\n",
    "    <li>They allow you to build <b>modular</b>, <b>reproducible</b>, and <b>scalable</b> workflows</li>\n",
    "    <li>Different processes can be parallelized and resource allocation optimized.</li>\n",
    "    <li>Popular among bioinformaticians across industries, making it a great transferable skill.</li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"https://snakemake.readthedocs.io/en/stable/\" target=\"_blank\"><b>Snakemake</b></a> has Python-based syntax and integrates well with conda environments, making it relatively easy to learn. \n",
    "\n",
    "Another popular workflow language is <a href=\"https://www.nextflow.io/\" target=\"_blank\"><b>Nextflow</b></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac20e4-43b2-4506-892c-eea7c005f36c",
   "metadata": {},
   "source": [
    "# Worfklow Components:\n",
    "\n",
    "There are 2 major components of a snakemake workflow:\n",
    "\n",
    "1. Rules\n",
    "2. Snakefile\n",
    "\n",
    "The rules define the steps of the workflow, and the snakefile defines the aggregate workflow. We'll get into the details below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f72d8-e3bc-4a86-a264-99469dc65d6d",
   "metadata": {},
   "source": [
    "# Rules\n",
    "\n",
    "Each step in a snakemake pipeline is called a <b>rule</b>.\n",
    "\n",
    "In addition to the exact code to run, each rule defines the input and output files, additional parameters, conda environments (if applicable).\n",
    "\n",
    "The following is template for a simple rule:\n",
    "\n",
    "```snakemake\n",
    "rule myrule:\n",
    "    input:\n",
    "        fName1 = \"path/to/inputfile\",\n",
    "        fName2 = \"path/to/other/inputfile\",\n",
    "    output:\n",
    "        output1 = \"path/to/outputfile\",\n",
    "        output2 = \"path/to/another/outputfile\",\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        somecommand {input.fName1} {output.output1}\n",
    "        somecommand {input.fName1} {output.output2}\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "The commands in the `shell` block are executed as they would be on the command line. You can also run Python itself, with the `run` block:\n",
    "\n",
    "```snakemake\n",
    "rule myrule:\n",
    "    input:\n",
    "        fName1 = \"path/to/inputfile\",\n",
    "        fName2 = \"path/to/other/inputfile\",\n",
    "    output:\n",
    "        output1 = \"path/to/outputfile\",\n",
    "        output2 = \"path/to/another/outputfile\",\n",
    "    run:\n",
    "        # this is a python command\n",
    "        print(f\"Filename 1: {input.fName1}, Filename 2: {input.fName2}\")\n",
    "```\n",
    "\n",
    "You can access all components of the `input`, `output`, and other blocks using the syntax `block_name.attribute`, without the `{}`. However, because I used an f-string above, I included the braces as required by the f-string format.\n",
    "\n",
    "Importantly, if you define an output file in the `output` block, that file must be generated by the rule, otherwise snakemake will give you an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714d3a2-2e26-4a48-914b-d534e06990ce",
   "metadata": {},
   "source": [
    "# Snakefile\n",
    "\n",
    "The rules that will be used in the workflow need to be imported, similar to the `#include` statement in C++ programs.\n",
    "\n",
    "After that, you define the desired outputs, like so:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = config[\"output_dir\"]\n",
    "\n",
    "# Import rules\n",
    "include: \"rules.smk\"\n",
    "\n",
    "# read in a dataframe of samples to process\n",
    "df_samples_runs = pd.read_csv(config['isolates_to_run'])\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        [f\"{output_dir}/{sample}/output_file.txt\" for sample in df_samples_runs['Sample'].values]\n",
    "\n",
    "```\n",
    "\n",
    "Here, I defined a list of samples to run the workflow on from a dataframe. Because Snakemake is python-based, you can import python packages like `pandas` and read them in as you normally would in python.\n",
    "\n",
    "Also note that I referenced a config file. The `config` variable is a global variable that is accessible in both the snakefile and the rules file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10612835-1a8e-49ee-8e96-f8a5973c575f",
   "metadata": {},
   "source": [
    "# Conda Integration\n",
    "\n",
    "Since many packages have different dependencies, it is useful to be able to have different virtual environments for different steps. Snakemake allows defining different conda environments like so:\n",
    "\n",
    "```snakemake\n",
    "rule myrule:\n",
    "    input:\n",
    "        fName1 = \"path/to/inputfile\",\n",
    "        fName2 = \"path/to/other/inputfile\",\n",
    "    output:\n",
    "        output1 = \"path/to/outputfile\",\n",
    "        output2 = \"path/to/another/outputfile\",\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        somecommand {input.fName1} {output.output1}\n",
    "        somecommand {input.fName1} {output.output2}\n",
    "    conda:\n",
    "        \"envs/myenv.yaml\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78322c9-9375-4ab0-82a2-3b8b162137b2",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Once you have all of your rules, you define the workflow by the desired outputs. Snakemake then back fills all the rules that must be run, depending on the inputs and outputs of each rule, to get to the user-specified outputs.\n",
    "\n",
    "To run a workflow from the command line, the basic syntax is\n",
    "\n",
    "```bash\n",
    "snakemake --snakefile snakefile --use-conda --conda-frontend conda --configfile config.yaml --cores 1\n",
    "\n",
    "```\n",
    "\n",
    "You MUST specify the number of cores, otherwise snakemake will give you an error.\n",
    "\n",
    "In the above code, I specified the snakefile and config file to use. You can also change things like the home directory for a workflow (so all relative paths will be relative to this directory) with the `--directory` flag and specify a different directory where conda environments are stored than in this home directory with the `--conda-prefix` flag.\n",
    "\n",
    "You can also set the maximum available RAM with something like `--resources mem_mb=8000`, meaning that there are 8 GB of RAM available. \n",
    "\n",
    "We won't use them, but there are some additional helpful flags:\n",
    "\n",
    "<ul>\n",
    "    <li><code>--unlock</code>: Snakemake locks a directory in which a workflow is running, so you can not run multiple workflows from the same directory to prevent potential conflicts in the output files. The directory is unlocked once the process completes. Use this flag to unlock a directory, for example, if a process was interrupted.</li>\n",
    "    <li><code>--rerun-incomplete</code>: If a snakemake process is interrupted and some steps (jobs) only partially finished, rerun those</li>\n",
    "    <li><code>--keep-going</code>: If one job fails, but there are other jobs that do not rely on its outputs, run them. This is useful if you have many samples, and one fails, but the workflow can continue processing the others.</li>\n",
    "    <li><code>--dry-run</code>: Perform a dry run of the worfklow to ensure that the graph of jobs is valid and that all required inputs are present.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084685a-6f32-48c2-92ca-5d5d9ea39a66",
   "metadata": {},
   "source": [
    "# Directed Acylic Graph (DAG) of Jobs\n",
    "\n",
    "To see all the rules that will be run in their order to get to the outputs, you can inspec the directed acyclic graph (DAG).\n",
    "\n",
    "To get the DAG for a particular process, you can run the following to save an image of the DAG.\n",
    "\n",
    "```bash\n",
    "snakemake --snakefile snakefile --use-conda --conda-frontend conda --configfile config.yaml --cores 1 --dag | dot -Tsvg > dag.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a05e60-da4d-4c61-b5c6-c9b62f78b595",
   "metadata": {},
   "source": [
    "# Activity: Short-Read Processing and Classification for <i>M. tuberculosis</i>\n",
    "\n",
    "In this short activity, we will download short-read whole-genome sequencing for <i>Mycobacterium tuberculosis</i>, perform quality-control, and perform taxonomic classification.\n",
    "\n",
    "Sequencing samples can be contaminated by DNA from humans (either the person from whom the sample was collected or anyone working in the laboratory), environmental bacteria, or other samples being tested in the laboratory. They could also be contaminated with other organisms if the patient is infected with multiple pathogens simultaneously.\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Get quality control statistics on read length and quality using `fastqc` and `seqkit`.\n",
    "2. Trim sequencing adapters and remove reads that are too short using `fastp`.\n",
    "4. Perform taxonomic classification (actually we won't do this because it is too memory-intensive, so I have provided the output files) using `kraken2`.\n",
    "\n",
    "After this, we will inspect the taxonomic classifications of reads to see if they make sense with what we expect.\n",
    "\n",
    "# 1. Notebook Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4b7fb7b-13ab-4816-9800-d5ef0efc5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f707fb-5a02-441c-add0-2525f3ef9731",
   "metadata": {},
   "source": [
    "# 2. Run the workflow\n",
    "\n",
    "Run the following commands to generate all the output files (except the kraken classification files, which I have provided already)\n",
    "\n",
    "```bash\n",
    "cd snakemake_tutorial_BST281\n",
    "\n",
    "# activate the environment\n",
    "conda activate snakemake_tutorial_BST281\n",
    "\n",
    "# change the number of cores and RAM available for your local machine\n",
    "snakemake --snakefile snakefile --use-conda --conda-frontend conda --configfile config.yaml --cores 8 --resources mem_mb=8000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd990faf-0b94-469c-a424-017230b0659e",
   "metadata": {},
   "source": [
    "# 3. Read Quality Control\n",
    "\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2dd38-c016-41bf-b342-9f9cbdff837d",
   "metadata": {},
   "source": [
    "### i. Plot the average quality of reads, stratified by sample and the read file (6 values total). Also plot the qualities scaled to error rates.\n",
    "\n",
    "### ii. What do you notice about the quality scoress for R1 vs. R2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f5d14-c460-406a-85ad-3aa1f8a96097",
   "metadata": {},
   "source": [
    "# 4. Taxonomic Classification\n",
    "\n",
    "For taxonomic classification, we will use the Kraken suite of tools for metagenomic classification from Johns Hopkins. \n",
    "\n",
    "These tools use exact-matching of read k-mers against a Kraken database of k-mers from existing genomes.\n",
    "\n",
    "For full read classification, kraken tools are preferred over other methods (like MetaPhlan) because they contain genome-wide k-mers, rather than only k-mers that distinguish between taxa. So they can be used for both sample-level classification and read-level classification.\n",
    "\n",
    "Kraken2, the software we will be using, is also very fast.\n",
    "\n",
    "More about it here:\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"https://www.nature.com/articles/s41596-022-00738-y\" target=\"_blank\">Metagenome analysis using the Kraken software suite\n",
    "</a></li>\n",
    "    <li><a href=\"https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0\" target=\"_blank\">Improved metagenomic analysis with Kraken 2</a></li>\n",
    "</ul>\n",
    "\n",
    "<b>In each kraken report, there are 6 columns:</b>\n",
    "\n",
    "1. Percentage of Reads: Percentage of total classified reads assigned to this taxon (including sub-taxa).\n",
    "2. Number of Reads: Number of reads assigned directly to this taxon (not including sub-taxa).\n",
    "3. Number of Reads (Including Sub-taxa): Number of reads assigned to this taxon plus all its sub-taxa.\n",
    "4. Taxonomic Rank Code: Single-letter code indicating taxonomic rank (e.g., U = Unclassified, R = Root, D = Domain, P = Phylum, C = Class, O = Order, F = Family, G = Genus, S = Species).\n",
    "5. NCBI Taxonomic ID: The NCBI Taxonomy ID for this taxon.\n",
    "6. Taxon Name: The name of the taxon, indented with leading spaces to show the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61cf6f-65ae-42bb-a794-49b77d7a760f",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### iii. What percent of reads map to the following groups in each sample:\n",
    "\n",
    "<ul>\n",
    "<li><i>Mycobacterium</i> genus</li>\n",
    "<li><i>Homo sapiens</i></li>\n",
    "<li>Unclassified</li>\n",
    "</ul>\n",
    "\n",
    "### iv. Where do you think the unclassified reads come from?\n",
    "\n",
    "### v. What is the most common bacterial contaminant (not in the <i>Mycobacterium</i> genus) in each sample?\n",
    "\n",
    "### vi. In any of the samples, is the most common genus not <i>Mycobacterium</i>?\n",
    "\n",
    "### vii. Where do you think these bacterial contaminants come from? Are they biological or technical?\n",
    "\n",
    "There was an interesting <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4228153/\" target=\"_blank\">paper</a> published some years ago about contaminating DNA found in PCR and DNA library prep reagents. Many of the organisms from which this DNA originates are commensal bacteria or ubiquitous environmental bacteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291dcd57-5ee4-4c88-b020-8db6c88b9297",
   "metadata": {},
   "source": [
    "# 5. Write your own rule\n",
    "\n",
    "Let's say we want to further investigate the unclassified reads to determine if they are something novel not represented in the standard Kraken database or if they are due to poor quality sequencing. The first step would be to gather the unclassified reads and their associated quality scores.\n",
    "\n",
    "## Questions\n",
    "\n",
    "### viii. How many reads are in each of the 6 FASTQ files (after the kraken-classification step)?\n",
    "\n",
    "Verify that the R1 and R2 files of each sample have the same numbers of reads.\n",
    "\n",
    "### ix. Extract the 1000th read (by count) from each file and print it and its name out.\n",
    "\n",
    "Do not use any tools other than Python (Biopython) and bash.\n",
    "\n",
    "### x. Write a rule to extract the unclassified reads for each sample and write them to new FASTQ files \n",
    "\n",
    "The `seqtk` <a href=\"https://github.com/lh3/seqtk\" target=\"_blank\">package</a> will be useful because it can extract reads by name. This package has already been installed in the `read_QC` environment.\n",
    "\n",
    "There should be two FASTQ files, one for the forward reads and one for the reverse. Add the rule to your `rules.smk` file. Update your `snakefile` accordingly and rerun the workflow. \n",
    "\n",
    "Notice that the workflow will only run the new rule because the previous rules nor their output files have been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aab23c-6120-4138-b619-c6c3a409c743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
